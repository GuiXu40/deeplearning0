{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcdGooNFjZ0cfMRi2Xs2Wm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuiXu40/deeplearning0/blob/main/Basic_code/%E5%8D%95%E5%90%91RNN%E5%92%8C%E5%8F%8C%E5%90%91RNN%E5%AE%9E%E7%8E%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFJi5XlGR-z1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "bjL8GWZ2SbGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "bs, T = 2, 3 #批量大小，和输入序列长度\n",
        "input_size, hidden_size = 2, 3 # 输入特征大小， 隐含层特征大小\n",
        "input = torch.randn(bs, T, input_size) # 随机初始化一个输入特征序列\n",
        "h_prev = torch.zeros(bs, hidden_size) # 初始隐含状态\n",
        "# step1 调用pyTorch RNN API https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "rnn_ouput, state_final = rnn(input, h_prev.unsqueeze(0))\n",
        "#print(\"Pytorch RNN API:\")\n",
        "#print(rnn_ouput)\n",
        "#print(state_final)\n",
        "\n",
        "# step2 手写一个rnn_forward函数，实现RNN原理的计算\n",
        "def rnn_forward(input, weight_ih, weight_hh, bias_ih, bias_hh, h_prev):\n",
        "  \"\"\"\n",
        "  input: [b, T, input_size]\n",
        "  weight_ih: [hidden_size, input_size]\n",
        "  weight_hh: [hidden_size, hidden_size]\n",
        "  bias_ih: [input_size]\n",
        "  bias_hh: [hidden_size]\n",
        "  h_prev: [b, hidden_size]\n",
        "  \"\"\"\n",
        "  bs, T, input_size = input.shape\n",
        "  h_dim = weight_ih.shape[0]\n",
        "  h_out = torch.zeros(bs, T, h_dim)\n",
        "\n",
        "  for t in range(T):\n",
        "    x = input[:, t, :].unsqueeze(2) # 获取当前时刻的输入特征， bs*input_size*1\n",
        "    w_ih_batch = weight_ih.unsqueeze(0).tile(bs, 1, 1) # b*h_dim*input_size\n",
        "    w_hh_batch = weight_hh.unsqueeze(0).tile(bs, 1, 1) # b*h_dim*h_dim\n",
        "\n",
        "    w_times_x = torch.bmm(w_ih_batch, x).squeeze(-1) # b*h_dim\n",
        "    w_times_h = torch.bmm(w_hh_batch, h_prev.unsqueeze(2)).squeeze(-1) # b*h_dim\n",
        "    h_prev = torch.tanh(w_times_x+bias_ih + w_times_h+bias_hh) # b*h_dim\n",
        "\n",
        "    h_out[:, t, :] = h_prev\n",
        "  return h_out, h_prev.unsqueeze(0) # b*T*h_dim 1*b*h_dim\n",
        "\n",
        "#for k, v in rnn.named_parameters():\n",
        "#  print(k, v)\n",
        "custom_run_output, custom_state_final = rnn_forward(input, rnn.weight_ih_l0, rnn.weight_hh_l0, rnn.bias_ih_l0, rnn.bias_hh_l0, h_prev)\n",
        "#print(\"\\n rnn_forward function output:\")\n",
        "#print(custom_run_output)\n",
        "#print(custom_state_final)\n",
        "\n",
        "# step3 手写衣蛾bidirectional_rnn_forward 函数， 实现双向RNN的计算原理\n",
        "def bidirectional_rnn_forward(input, weight_ih, weight_hh, bias_ih, bias_hh, h_prev, \\\n",
        "                               weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse):\n",
        "  \"\"\"\n",
        "  input: [b, T, input_size]\n",
        "  weight_ih: [hidden_size, input_size]\n",
        "  weight_hh: [hidden_size, hidden_size]\n",
        "  bias_ih: [input_size]\n",
        "  bias_hh: [hidden_size]\n",
        "  h_prev: [b, hidden_size]\n",
        "  \"\"\"\n",
        "  bs, T, input_size = input.shape\n",
        "  h_dim = weight_ih.shape[0]\n",
        "  h_out = torch.zeros(bs, T, h_dim*2) # 初始化一个输出（状态）矩阵，注意双向是两倍的特征大小\n",
        "\n",
        "  forward_output = rnn_forward(input, weight_ih, weight_hh, bias_ih, bias_hh, h_prev)[0]\n",
        "  backward_output = rnn_forward(torch.flip(input, [1]), weight_ih_reverse, weight_hh_reverse, bias_ih_reverse, bias_hh_reverse, h_prev_reverse)[0]\n",
        "\n",
        "  h_out[:, :, :h_dim] = forward_output\n",
        "  h_out[:, :, h_dim:] = torch.flip(backward_output, [1])\n",
        "\n",
        "  h_n = torch.zeros(bs, 2, h_dim)\n",
        "  h_n[:, 0, :] = forward_output[:, -1, :]\n",
        "  h_n[:, 1, :] = backward_output[:, -1, :]\n",
        "  h_n = h_n.transpose(0, 1)\n",
        "\n",
        "  #return h_out, h_out[:, -1, :].reshape((bs, 2, h_dim)).transpose(0, 1)\n",
        "  return h_out, h_n\n",
        "\n",
        "# 验证bidirectional_rnn_function 的正确性\n",
        "bi_rnn = nn.RNN(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "h_prev = torch.zeros(2, bs, hidden_size)\n",
        "bi_rnn_output, bi_state_final = bi_rnn(input, h_prev)\n",
        "#for k, v in bi_rnn.named_parameters():\n",
        "#  print(k, v)\n",
        "print(\"pytorch bi_RNN API:\")\n",
        "print(bi_rnn_output)\n",
        "print(bi_state_final)\n",
        "\n",
        "custom_bi_run_output, custom_bi_state_final = bidirectional_rnn_forward(input, bi_rnn.weight_ih_l0, bi_rnn.weight_hh_l0, bi_rnn.bias_ih_l0, bi_rnn.bias_hh_l0, h_prev[0], \\\n",
        "                                                                        bi_rnn.weight_ih_l0_reverse, bi_rnn.weight_hh_l0_reverse, \\\n",
        "                                                                        bi_rnn.bias_ih_l0_reverse, bi_rnn.bias_hh_l0_reverse, h_prev[1])\n",
        "\n",
        "print(\"\\n bidirectional_rnn_forward output:\")\n",
        "print(custom_bi_run_output)\n",
        "print(custom_bi_state_final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1o0GuwCSdXG",
        "outputId": "cabaa911-caa0-4307-f36e-b5fc32bcddac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pytorch bi_RNN API:\n",
            "tensor([[[ 0.6269,  0.2766,  0.3033, -0.3628,  0.5605,  0.0917],\n",
            "         [ 0.8897, -0.1003,  0.0740, -0.5450, -0.1588,  0.0874],\n",
            "         [ 0.8579,  0.1222,  0.4492, -0.5948,  0.4981, -0.0735]],\n",
            "\n",
            "        [[ 0.7717,  0.4923,  0.2075, -0.3371,  0.7338, -0.3508],\n",
            "         [ 0.5323, -0.5082,  0.3746, -0.0385,  0.6485, -0.2444],\n",
            "         [ 0.8457,  0.1279,  0.4327, -0.5614,  0.3480, -0.1893]]],\n",
            "       grad_fn=<TransposeBackward1>)\n",
            "tensor([[[ 0.8579,  0.1222,  0.4492],\n",
            "         [ 0.8457,  0.1279,  0.4327]],\n",
            "\n",
            "        [[-0.3628,  0.5605,  0.0917],\n",
            "         [-0.3371,  0.7338, -0.3508]]], grad_fn=<StackBackward0>)\n",
            "\n",
            " bidirectional_rnn_forward output:\n",
            "tensor([[[ 0.6269,  0.2766,  0.3033, -0.3628,  0.5605,  0.0917],\n",
            "         [ 0.8897, -0.1003,  0.0740, -0.5450, -0.1588,  0.0874],\n",
            "         [ 0.8579,  0.1222,  0.4492, -0.5948,  0.4981, -0.0735]],\n",
            "\n",
            "        [[ 0.7717,  0.4923,  0.2075, -0.3371,  0.7338, -0.3508],\n",
            "         [ 0.5323, -0.5082,  0.3746, -0.0385,  0.6485, -0.2444],\n",
            "         [ 0.8457,  0.1279,  0.4327, -0.5614,  0.3480, -0.1893]]],\n",
            "       grad_fn=<CopySlices>)\n",
            "tensor([[[ 0.8579,  0.1222,  0.4492],\n",
            "         [ 0.8457,  0.1279,  0.4327]],\n",
            "\n",
            "        [[-0.3628,  0.5605,  0.0917],\n",
            "         [-0.3371,  0.7338, -0.3508]]], grad_fn=<TransposeBackward0>)\n"
          ]
        }
      ]
    }
  ]
}