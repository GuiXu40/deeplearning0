{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1EdGsK2uP0hcfka97Cx_ssq15PtD_Vxko",
      "authorship_tag": "ABX9TyPbJJIFYZZcmZSFNjpAWHvQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuiXu40/deeplearning0/blob/main/Basic_code/Dlow%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/001_%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86%E4%BB%A3%E7%A0%81%E7%90%86%E8%A7%A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4krw8_ECC97",
        "outputId": "131b788e-3aba-44d0-cb85-1a4d149c8b45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/ColabNotebooks/DLow-master/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2IfAKBmCXQf",
        "outputId": "54ce80d4-3569-4781-dbda-ad8eecb45b3a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ColabNotebooks/DLow-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE_Wz77NCg79",
        "outputId": "a16aba10-c734-4e83-d2b0-639febcb7995"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  LICENSE  models  motion_pred  README.md  results\tUntitled0.ipynb  utils\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!vim ./motion_pred/utils/dataset_h36m.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzgZMNfTDXqX",
        "outputId": "f532d19f-3d99-4ea6-99b6-bffe0e7f1b4d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2004h\u001b7\u001b[?47h\u001b[?1h\u001b=\u001b[?2004h\u001b[1;24r\u001b[?12h\u001b[?12l\u001b[29m\u001b[m\u001b[H\u001b[2J\u001b[?25l\u001b[24;1H\"./motion_pred/utils/dataset_h36m.py\" 62L, 2646C\u001b[2;1H▽\u001b[6n\u001b[2;1H  \u001b[1;1H\u001b[>c\u001b]10;?\u0007\u001b]11;?\u0007\u001b[1;1H\u001b[35mimport\u001b[m numpy \u001b[33mas\u001b[m np\n",
            "\u001b[35mimport\u001b[m os\n",
            "\u001b[35mfrom\u001b[m motion_pred.utils.dataset \u001b[35mimport\u001b[m Dataset\n",
            "\u001b[35mfrom\u001b[m motion_pred.utils.skeleton \u001b[35mimport\u001b[m Skeleton\n",
            "\n",
            "\n",
            "\u001b[33mclass\u001b[m \u001b[36mDatasetH36M\u001b[m(Dataset):\u001b[9;5H\u001b[33mdef\u001b[m \u001b[36m__init__\u001b[m(self, mode, t_his=\u001b[31m25\u001b[m, t_pred=\u001b[31m100\u001b[m, actions=\u001b[31m'all'\u001b[m, use_vel=\u001b[36mFalse\u001b[m))\u001b[10;1H:\u001b[11;9Hself.use_vel = use_vel\u001b[12;9H\u001b[36msuper\u001b[m().__init__(mode, t_his, t_pred, actions)\u001b[13;9H\u001b[33mif\u001b[m use_vel:\u001b[14;13Hself.traj_dim += \u001b[31m3\u001b[m\u001b[16;5H\u001b[33mdef\u001b[m \u001b[36mprepare_data\u001b[m(self):\u001b[17;9Hself.data_file = os.path.join(\u001b[31m'data'\u001b[m, \u001b[31m'data_3d_h36m.npz'\u001b[m)\u001b[18;9Hself.subjects_split = {\u001b[31m'train'\u001b[m: [\u001b[31m1\u001b[m, \u001b[31m5\u001b[m, \u001b[31m6\u001b[m, \u001b[31m7\u001b[m, \u001b[31m8\u001b[m],\u001b[19;32H\u001b[31m'test'\u001b[m: [\u001b[31m9\u001b[m, \u001b[31m11\u001b[m]}\u001b[20;9Hself.subjects = [\u001b[31m'S%d'\u001b[m % x \u001b[33mfor\u001b[m x \u001b[33min\u001b[m self.subjects_split[self.mode]]\u001b[21;9Hself.skeleton = Skeleton(parents=[-\u001b[31m1\u001b[m, \u001b[31m0\u001b[m, \u001b[31m1\u001b[m, \u001b[31m2\u001b[m, \u001b[31m3\u001b[m, \u001b[31m4\u001b[m, \u001b[31m0\u001b[m, \u001b[31m6\u001b[m, \u001b[31m7\u001b[m, \u001b[31m8\u001b[m, \u001b[31m9\u001b[m, \u001b[31m0\u001b[m, \u001b[31m11\u001b[22;1H1\u001b[m, \u001b[31m12\u001b[m, \u001b[31m13\u001b[m, \u001b[31m14\u001b[m, \u001b[31m12\u001b[m,\u001b[23;43H\u001b[31m16\u001b[m, \u001b[31m17\u001b[m, \u001b[31m18\u001b[m, \u001b[31m19\u001b[m, \u001b[31m20\u001b[m, \u001b[31m19\u001b[m, \u001b[31m22\u001b[m, \u001b[31m12\u001b[m, \u001b[31m24\u001b[m, \u001b[31m25\u001b[m\u001b[23;1H\u001b[1m\u001b[34m@@@                                                                             \u001b[m\u001b[24;63H1,1\u001b[11CTop\u001b[1;1H\u001b[?25h\u001b[24;1H\u001b[?2004l\u001b[?1l\u001b>\u001b[2J\u001b[?47l\u001b8Vim: Caught deadly signal TERM\n",
            "\n",
            "Vim: Finished.\n",
            "\u001b[24;1H^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "h36 = np.load('./data/data_3d_h36m.npz',allow_pickle=True)"
      ],
      "metadata": {
        "id": "vJBxI-mmJ-ng"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h36.files\n",
        "h36['positions_3d']"
      ],
      "metadata": {
        "id": "EOyNfaJYKRAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_o = np.load('./data/data_3d_h36m.npz', allow_pickle=True)['positions_3d'].item()\n",
        "#for k in data_o:\n",
        "#  print(k)\n",
        "# 每个文件夹 代表一个人， 这里只有7个，因为只有这7个人有3D 标签。\n",
        "\n",
        "# 每个人的动作场景\n",
        "#for k in list(dict(data_o['S1'].items()).keys()):\n",
        "#  print(k)\n",
        "\n",
        "#print(dict(data_o['S1'].items())['Eating'].shape)\n",
        "#print(dict(data_o['S1'].items())['Eating'])\n",
        "\n",
        "#dict(data_o['S1'].items())['Eating']\n",
        "a = np.array([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 10],\n",
        "    [7, 8, 9]\n",
        "])\n",
        "b = np.array([\n",
        "    [1, 2],\n",
        "    [4, 4],\n",
        "    [7, 8]\n",
        "])\n",
        "\n",
        "print(a[:, 1:] - a[:, :1])\n",
        "print(b[:, 1:] - b[:, :1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtC6Z5NZbYsG",
        "outputId": "d784686d-d1e0-423c-90d9-1817af6d1b26"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2]\n",
            " [1 6]\n",
            " [1 2]]\n",
            "[[1]\n",
            " [0]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. dataset_h36m代码理解\n",
        "该类继承自 dataset 类。"
      ],
      "metadata": {
        "id": "rax7U6TLNqCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 dataset类代码"
      ],
      "metadata": {
        "id": "zjd4UsxDOLvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "    def __init__(self, mode, t_his, t_pred, actions='all'):\n",
        "        \"\"\"\n",
        "          mode: 训练模式或者测试模式\n",
        "          t_his: 历史t时间步\n",
        "          t_prep: 待预测的时间步\n",
        "          actions: 具体的场景\n",
        "        \"\"\"\n",
        "        self.mode = mode\n",
        "        self.t_his = t_his\n",
        "        self.t_pred = t_pred\n",
        "        self.t_total = t_his + t_pred # 总时间步长\n",
        "        self.actions = actions\n",
        "        self.prepare_data() # 准备数据\n",
        "        self.std, self.mean = None, None\n",
        "        self.data_len = sum([seq.shape[0] for data_s in self.data.values() for seq in data_s.values()]) # data:[S*, 值] data_s[动作，值]\n",
        "        self.traj_dim = (self.kept_joints.shape[0] - 1) * 3\n",
        "        self.normalized = False\n",
        "        # iterator specific\n",
        "        self.sample_ind = None\n",
        "\n",
        "    # 数据准备方法， 待实现\n",
        "    def prepare_data(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # 数据归一化\n",
        "    def normalize_data(self, mean=None, std=None):\n",
        "        if mean is None:\n",
        "            all_seq = []\n",
        "            for data_s in self.data.values():\n",
        "                for seq in data_s.values():\n",
        "                    all_seq.append(seq[:, 1:])\n",
        "            all_seq = np.concatenate(all_seq)\n",
        "            self.mean = all_seq.mean(axis=0)\n",
        "            self.std = all_seq.std(axis=0)\n",
        "        else:\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "        for data_s in self.data.values():\n",
        "            for action in data_s.keys():\n",
        "                data_s[action][:, 1:] = (data_s[action][:, 1:] - self.mean) / self.std\n",
        "        self.normalized = True\n",
        "\n",
        "    # 随机采样 t_total 时间步长的动作序列\n",
        "    def sample(self):\n",
        "        subject = np.random.choice(self.subjects) # 生成随机数 subjects: s*\n",
        "        dict_s = self.data[subject]\n",
        "        action = np.random.choice(list(dict_s.keys()))\n",
        "        seq = dict_s[action]\n",
        "        fr_start = np.random.randint(seq.shape[0] - self.t_total)\n",
        "        fr_end = fr_start + self.t_total\n",
        "        traj = seq[fr_start: fr_end]\n",
        "        return traj[None, ...]\n",
        "\n",
        "    # dataloader batch_size=8, num_samples=1000: 采样总数\n",
        "    def sampling_generator(self, num_samples=1000, batch_size=8):\n",
        "        for i in range(num_samples // batch_size):\n",
        "            sample = []\n",
        "            for i in range(batch_size):\n",
        "                sample_i = self.sample()\n",
        "                sample.append(sample_i)\n",
        "            sample = np.concatenate(sample, axis=0)\n",
        "            yield sample # (b, T, 关节点数, 坐标维度)\n",
        "\n",
        "    # 动作序列迭代器\n",
        "    def iter_generator(self, step=25):\n",
        "        for data_s in self.data.values():\n",
        "            for seq in data_s.values():\n",
        "                seq_len = seq.shape[0]\n",
        "                for i in range(0, seq_len - self.t_total, step):\n",
        "                    traj = seq[None, i: i + self.t_total]\n",
        "                    yield traj\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mQodp9QfOQEq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Skeleton类"
      ],
      "metadata": {
        "id": "yYZjMvlkOsEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2018-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "#\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class Skeleton:\n",
        "    def __init__(self, parents, joints_left, joints_right):\n",
        "        assert len(joints_left) == len(joints_right)\n",
        "        \n",
        "        self._parents = np.array(parents)\n",
        "        self._joints_left = joints_left\n",
        "        self._joints_right = joints_right\n",
        "        self._compute_metadata()\n",
        "    \n",
        "    # 返回关节点数\n",
        "    def num_joints(self):\n",
        "        return len(self._parents)\n",
        "    \n",
        "    # 返回父关节点\n",
        "    def parents(self):\n",
        "        return self._parents\n",
        "    \n",
        "    # 返回是否有子节点\n",
        "    def has_children(self):\n",
        "        return self._has_children\n",
        "\n",
        "    # 返回子关节点\n",
        "    def children(self):\n",
        "        return self._children\n",
        "    \n",
        "    # 移除某些关节点\n",
        "    def remove_joints(self, joints_to_remove):\n",
        "        \"\"\"\n",
        "        Remove the joints specified in 'joints_to_remove'.\n",
        "        \"\"\"\n",
        "        valid_joints = []\n",
        "\n",
        "        # 保留不需要移除的关节点\n",
        "        for joint in range(len(self._parents)):\n",
        "            if joint not in joints_to_remove:\n",
        "                valid_joints.append(joint)\n",
        "\n",
        "        # 如果父节点被删除，就修改_parents，直到找到父节点\n",
        "        for i in range(len(self._parents)):\n",
        "            while self._parents[i] in joints_to_remove:\n",
        "                self._parents[i] = self._parents[self._parents[i]]\n",
        "\n",
        "        # 移除多余的空位       \n",
        "        index_offsets = np.zeros(len(self._parents), dtype=int)\n",
        "        new_parents = []\n",
        "        for i, parent in enumerate(self._parents):\n",
        "            if i not in joints_to_remove:\n",
        "                new_parents.append(parent - index_offsets[parent])\n",
        "            else:\n",
        "                index_offsets[i:] += 1\n",
        "        self._parents = np.array(new_parents)\n",
        "        \n",
        "        # 调整 左右关节点的位置\n",
        "        if self._joints_left is not None:\n",
        "            new_joints_left = []\n",
        "            for joint in self._joints_left:\n",
        "                if joint in valid_joints:\n",
        "                    new_joints_left.append(joint - index_offsets[joint])\n",
        "            self._joints_left = new_joints_left\n",
        "        if self._joints_right is not None:\n",
        "            new_joints_right = []\n",
        "            for joint in self._joints_right:\n",
        "                if joint in valid_joints:\n",
        "                    new_joints_right.append(joint - index_offsets[joint])\n",
        "            self._joints_right = new_joints_right\n",
        "\n",
        "        self._compute_metadata()\n",
        "        \n",
        "        return valid_joints\n",
        "    \n",
        "    # 返回左关节点\n",
        "    def joints_left(self):\n",
        "        return self._joints_left\n",
        "    \n",
        "    def joints_right(self):\n",
        "        return self._joints_right\n",
        "        \n",
        "    def _compute_metadata(self):\n",
        "        self._has_children = np.zeros(len(self._parents)).astype(bool)\n",
        "        for i, parent in enumerate(self._parents):\n",
        "            if parent != -1:\n",
        "                self._has_children[parent] = True\n",
        "\n",
        "        self._children = []\n",
        "        for i, parent in enumerate(self._parents):\n",
        "            self._children.append([])\n",
        "        for i, parent in enumerate(self._parents):\n",
        "            if parent != -1:\n",
        "                self._children[parent].append(i)"
      ],
      "metadata": {
        "id": "xOMXDpKHOupd"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 测试skeleton 类\n",
        "skeleton = Skeleton(parents=[-1, 0, 1, 2, 3, 4, 0, 6, 7, 8, 9, 0, 11, 12, 13, 14, 12,\n",
        "                                  16, 17, 18, 19, 20, 19, 22, 12, 24, 25, 26, 27, 28, 27, 30],\n",
        "                          joints_left=[6, 7, 8, 9, 10, 16, 17, 18, 19, 20, 21, 22, 23],\n",
        "                          joints_right=[1, 2, 3, 4, 5, 24, 25, 26, 27, 28, 29, 30, 31])\n",
        "removed_joints = {4, 5, 9, 10, 11, 16, 20, 21, 22, 23, 24, 28, 29, 30, 31}\n",
        "kept_joints = np.array([x for x in range(32) if x not in removed_joints])\n",
        "skeleton.remove_joints(removed_joints)\n",
        "\n",
        "# 1.关节点数\n",
        "print(skeleton.num_joints())\n",
        "# 2.父节点\n",
        "print(skeleton.parents())\n",
        "# 3.是否有子节点\n",
        "print(skeleton.has_children())\n",
        "# 4.返回子节点\n",
        "print(skeleton.children())\n",
        "# 5.返回左关节点\n",
        "print(skeleton.joints_left())\n",
        "# 6.返回右关节点\n",
        "print(skeleton.joints_right())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HwSD-2FOzP3",
        "outputId": "7723c24b-507d-49cc-d846-5a9a83701ed2"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17\n",
            "[-1  0  1  2  0  4  5  0  7  8  9  7 11 12  7 14 15]\n",
            "[ True  True  True False  True  True False  True  True  True False  True\n",
            "  True False  True  True False]\n",
            "[[1, 4, 7], [2], [3], [], [5], [6], [], [8, 11, 14], [9], [10], [], [12], [13], [], [15], [16], []]\n",
            "[4, 5, 6, 11, 12, 13]\n",
            "[1, 2, 3, 14, 15, 16]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 dataset_h36m"
      ],
      "metadata": {
        "id": "eHbiK4R8O0Ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "#from motion_pred.utils.dataset import Dataset\n",
        "#from motion_pred.utils.skeleton import Skeleton\n",
        "\n",
        "\n",
        "class DatasetH36M(Dataset):\n",
        "\n",
        "    def __init__(self, mode, t_his=25, t_pred=100, actions='all', use_vel=False):\n",
        "        self.use_vel = use_vel\n",
        "        super().__init__(mode, t_his, t_pred, actions)\n",
        "        if use_vel:\n",
        "            self.traj_dim += 3\n",
        "\n",
        "    # 从文件中读取数据\n",
        "    def prepare_data(self):\n",
        "        self.data_file = os.path.join('data', 'data_3d_h36m.npz')\n",
        "        self.subjects_split = {'train': [1, 5, 6, 7, 8],\n",
        "                               'test': [9, 11]}\n",
        "        self.subjects = ['S%d' % x for x in self.subjects_split[self.mode]]\n",
        "        self.skeleton = Skeleton(parents=[-1, 0, 1, 2, 3, 4, 0, 6, 7, 8, 9, 0, 11, 12, 13, 14, 12,\n",
        "                                          16, 17, 18, 19, 20, 19, 22, 12, 24, 25, 26, 27, 28, 27, 30],\n",
        "                                 joints_left=[6, 7, 8, 9, 10, 16, 17, 18, 19, 20, 21, 22, 23],\n",
        "                                 joints_right=[1, 2, 3, 4, 5, 24, 25, 26, 27, 28, 29, 30, 31])\n",
        "        self.removed_joints = {4, 5, 9, 10, 11, 16, 20, 21, 22, 23, 24, 28, 29, 30, 31}\n",
        "        self.kept_joints = np.array([x for x in range(32) if x not in self.removed_joints])\n",
        "        self.skeleton.remove_joints(self.removed_joints)\n",
        "        self.skeleton._parents[11] = 8\n",
        "        self.skeleton._parents[14] = 8\n",
        "        self.process_data()\n",
        "\n",
        "    def process_data(self):\n",
        "        data_o = np.load(self.data_file, allow_pickle=True)['positions_3d'].item()\n",
        "        # 过滤出数据， filter(function, iterable) return list  然后 dict 转为字典\n",
        "        data_f = dict(filter(lambda x: x[0] in self.subjects, data_o.items())) # 过滤出我们需要的数据 e.g. train:[1, 5, 6, 7, 8] 丢掉 9,11\n",
        "\n",
        "        if self.actions != 'all':\n",
        "            for key in list(data_f.keys()): # ['S1',...,'S8']\n",
        "                data_f[key] = dict(filter(lambda x: all([a in x[0] for a in self.actions]), data_f[key].items())) # 筛选出需要的actions\n",
        "                if len(data_f[key]) == 0: # 如果为0 说明没有该动作场景的动作，删除该人。\n",
        "                    data_f.pop(key)\n",
        "        for data_s in data_f.values(): # data_f.values(): [动作：数据]\n",
        "            # 数据处理，最后一维的插值\n",
        "            for action in data_s.keys(): # data_s.keys() : 动作\n",
        "                seq = data_s[action][:, self.kept_joints, :] # data_s[action]: (2721, 32, 3)\n",
        "                if self.use_vel:\n",
        "                    v = (np.diff(seq[:, :1], axis=0) * 50).clip(-5.0, 5.0)\n",
        "                    v = np.append(v, v[[-1]], axis=0)\n",
        "                seq[:, 1:] -= seq[:, :1]\n",
        "                if self.use_vel:\n",
        "                    seq = np.concatenate((seq, v), axis=1)\n",
        "                data_s[action] = seq\n",
        "        self.data = data_f\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(0)\n",
        "    actions = {'WalkDog'}\n",
        "    dataset = DatasetH36M('train', actions=actions)\n",
        "    generator = dataset.sampling_generator()\n",
        "    dataset.normalize_data()\n",
        "    # generator = dataset.iter_generator()\n",
        "    for data in generator:\n",
        "        print(data.shape)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P8fRdfwoO3b6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 总结\n",
        "数据预处理部分，可以看出动作序列分为，S* 一共7个人，每一个人中有不同的动作序列，维度为(T, joints_num, joints_dim), 通过处理之后，可以返回(batch_size, T', joints_num', joints_dim) ,其中T'指的是我们需要的时间步长，包括历史序列，和对应的ground truth(未来序列)。joints_num'表示去除部分关节点之后的关节点数。"
      ],
      "metadata": {
        "id": "Fvem5jaMHaMF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LIY_SYtYHZtV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}