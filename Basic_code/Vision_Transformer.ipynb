{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpOEfTtgfApBZ16SMfs9OL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuiXu40/deeplearning0/blob/main/Basic_code/Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hStTVrB7US2o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step1. image patch embedding\n",
        "# 使用两种方式对image进行处理，一种是直接通过分割图片的方式。一种是通过卷积操作\n",
        "#  1. 直接分割图片\n",
        "def image2emb_naive(image, patch_size, weight):\n",
        "  patch = nn.functional.unfold(image, kernel_size=patch_size, stride=patch_size).transpose(-1, -2)\n",
        "  print(patch.shape)\n",
        "  patch_embedding = patch @ weight\n",
        "  return patch_embedding\n",
        "\n",
        "#  2. 使用卷积的方式直接输出\n",
        "def image2emb_conv(image, kernel, stride):\n",
        "  conv_output = F.conv2d(image, kernel, stride=stride)\n",
        "  bs, oc, oh, ow = conv_output.shape\n",
        "  patch_embedding = conv_output.reshape((bs, oc, oh * ow)).transpose(-1, -2)\n",
        "  return patch_embedding\n",
        "\n",
        "bs, ic, image_h, image_w = 1, 3, 8, 8\n",
        "patch_size = 4\n",
        "model_dim = 8\n",
        "patch_depth = patch_size * patch_size * ic\n",
        "image = torch.randn(bs, ic, image_h, image_w)\n",
        "weight = torch.randn(patch_depth, model_dim)\n",
        "\n",
        "patch_embedding_naive = image2emb_naive(image, patch_size, weight)\n",
        "print(patch_embedding_naive)\n",
        "\n",
        "kernel = weight.transpose(0, 1).reshape(-1, ic, patch_size, patch_size)\n",
        "patch_embedding_conv = image2emb_conv(image, kernel, patch_size)\n",
        "\n",
        "print(patch_embedding_conv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPPnYwkhUoZ-",
        "outputId": "769df54f-1223-42dc-86ef-0ce3a715737f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 4, 48])\n",
            "tensor([[[  2.9560,   3.5471,   1.9480,  -6.8817,  -4.3895,  -1.1609,   8.9942,\n",
            "           -1.1096],\n",
            "         [  1.8633,  -0.6170,   6.6276,   2.8051,   2.7494,   8.0640,   7.7082,\n",
            "          -11.2505],\n",
            "         [  3.1121, -10.1775,   7.9735,  -6.0336,  -0.3792,  -5.2678, -12.5721,\n",
            "            1.4477],\n",
            "         [ -3.5535,  10.2715,   4.5857,  -2.0812,  -4.3538,   1.0491,   0.4134,\n",
            "           -2.9734]]])\n",
            "tensor([[[  2.9560,   3.5471,   1.9480,  -6.8817,  -4.3895,  -1.1609,   8.9942,\n",
            "           -1.1096],\n",
            "         [  1.8633,  -0.6170,   6.6276,   2.8051,   2.7494,   8.0640,   7.7082,\n",
            "          -11.2505],\n",
            "         [  3.1121, -10.1775,   7.9736,  -6.0336,  -0.3792,  -5.2678, -12.5721,\n",
            "            1.4477],\n",
            "         [ -3.5535,  10.2715,   4.5857,  -2.0812,  -4.3538,   1.0491,   0.4134,\n",
            "           -2.9734]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step2. prepend CLS token embedding\n",
        "cls_token_embedding = torch.randn(bs, 1, model_dim, requires_grad = True)\n",
        "token_embedding = torch.cat([cls_token_embedding, patch_embedding_conv], dim = 1)\n",
        "print(token_embedding.shape)\n",
        "print(cls_token_embedding.shape)\n",
        "print(patch_embedding_conv.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI0C9JpFLm_A",
        "outputId": "4d9a5d03-8f2c-4b9d-951f-8baa91801c05"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 8])\n",
            "torch.Size([1, 1, 8])\n",
            "torch.Size([1, 4, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_num_token = 16\n",
        "# step3. add position embedding\n",
        "position_embedding_table = torch.randn(max_num_token, model_dim, requires_grad=True)\n",
        "seq_len = token_embedding.shape[1]\n",
        "print(position_embedding_table[:seq_len])\n",
        "position_embedding = torch.tile(position_embedding_table[:seq_len], [token_embedding.shape[0], 1, 1])\n",
        "token_embedding += position_embedding\n",
        "print(position_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va-TBOFPM-9x",
        "outputId": "8ba3255b-7a90-4ec2-92f0-adc95e4a3257"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.6283, -0.8537, -0.3089, -0.1871, -2.3427, -2.3121,  0.6822, -0.9336],\n",
            "        [-1.2068,  0.2422,  1.2009,  0.3887,  0.5662, -0.0687,  0.7246,  1.3458],\n",
            "        [ 0.0571, -1.1050,  0.5187, -0.5592, -1.0866, -0.1405,  1.2848, -1.2199],\n",
            "        [ 2.2285, -0.9752,  0.3891, -0.1614,  0.2204,  0.8927,  1.5179, -0.8595],\n",
            "        [-0.7251,  0.0619, -1.2961, -1.8820,  0.0202,  0.0031, -0.1315,  0.5817]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "torch.Size([1, 5, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# step4. pass embedding to Transformer Encoder\n",
        "encoder_layer = nn.TransformerEncoderLayer(d_model = model_dim, nhead=8)\n",
        "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "encoder_output = transformer_encoder(token_embedding)"
      ],
      "metadata": {
        "id": "mRqx67jOP_Z1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step5. do classification\n",
        "num_classes=10\n",
        "label = torch.randint(10, (bs,))\n",
        "\n",
        "cls_token_output = encoder_output[:, 0, :]\n",
        "linear_layer = nn.Linear(model_dim, num_classes)\n",
        "logits = linear_layer(cls_token_output)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loss = loss_fn(logits, label)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy2SisIRQhIY",
        "outputId": "565ebaca-9a30-4f6b-af28-33e54b303a38"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.3791, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    }
  ]
}