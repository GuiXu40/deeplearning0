{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPK8GraugsOLR0AA+1u6DD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GuiXu40/deeplearning0/blob/main/Basic_code/Dlow%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/002_exp_vae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyDUf7DXgJFC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. exp_vae.py 理解\n",
        "该文件是用vae模型预处理的训练文件。"
      ],
      "metadata": {
        "id": "Z8fopx_VgWSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os # 文件操作\n",
        "import sys # 程序与python解释器的交互\n",
        "import math\n",
        "import pickle # 序列化对象\n",
        "import argparse # 解析命令行参数\n",
        "import time\n",
        "from torch import optim\n",
        "from torch.utils.tensorboard import SummaryWriter # 可视化工具\n",
        "\n",
        "sys.path.append(os.getcwd())\n",
        "from utils import *\n",
        "from motion_pred.utils.config import Config\n",
        "from motion_pred.utils.dataset_h36m import DatasetH36M\n",
        "from motion_pred.utils.dataset_humaneva import DatasetHumanEva\n",
        "from models.motion_pred import *\n",
        "\n",
        "# 损失函数\n",
        "def loss_function(X, Y_r, Y, mu, logvar):\n",
        "    MSE = (Y_r - Y).pow(2).sum() / Y.shape[1]\n",
        "    MSE_v = (X[-1] - Y_r[0]).pow(2).sum() / Y.shape[1]\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / Y.shape[1]\n",
        "    loss_r = MSE + cfg.lambda_v * MSE_v + cfg.beta * KLD\n",
        "    return loss_r, np.array([loss_r.item(), MSE.item(), MSE_v.item(), KLD.item()])\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "    \"\"\"\n",
        "      epoch: 训练批次\n",
        "    \"\"\"\n",
        "    t_s = time.time() # 计算时间\n",
        "    train_losses = 0  # 终loss\n",
        "    total_num_sample = 0 # 总的采样数\n",
        "    loss_names = ['TOTAL', 'MSE', 'MSE_v', 'KLD']\n",
        "    generator = dataset.sampling_generator(num_samples=cfg.num_vae_data_sample, batch_size=cfg.batch_size) # 数据迭代器 (batch_size, T', joints_num, tra_dim)\n",
        "    for traj_np in generator:\n",
        "        traj_np = traj_np[..., 1:, :].reshape(traj_np.shape[0], traj_np.shape[1], -1) # traj_np: [batch_size, T', (joints_num-1)*tra_dim]\n",
        "        traj = tensor(traj_np, device=device, dtype=dtype).permute(1, 0, 2).contiguous() # permute: 调整tensor维度；contiguous: 会复制一份tensor； traj: (T', batch_size, (joints_num-1)*tra_dim)\n",
        "        X = traj[:t_his] # 获取历史序列\n",
        "        Y = traj[t_his:] # 获取ground truth, 未来序列\n",
        "        Y_r, mu, logvar = model(X, Y) # 传入模型\n",
        "        loss, losses = loss_function(X, Y_r, Y, mu, logvar) # 计算损失函数\n",
        "        optimizer.zero_grad() # 梯度清零\n",
        "        loss.backward() # 梯度反向计算\n",
        "        optimizer.step() # 更新参数\n",
        "        train_losses += losses\n",
        "        total_num_sample += 1 # 增加采样数\n",
        "\n",
        "    scheduler.step() # 更新学习率\n",
        "    dt = time.time() - t_s # 计算训练时间\n",
        "    train_losses /= total_num_sample # 计算平均损失\n",
        "    lr = optimizer.param_groups[0]['lr'] # 获取学习率\n",
        "    losses_str = ' '.join(['{}: {:.4f}'.format(x, y) for x, y in zip(loss_names, train_losses)])\n",
        "    logger.info('====> Epoch: {} Time: {:.2f} {} lr: {:.5f}'.format(epoch, dt, losses_str, lr)) # 输出日志， 训练批次：训练时间：损失：学习率\n",
        "    for name, loss in zip(loss_names, train_losses): # 保存到tensorboard 方便可视化\n",
        "        tb_logger.add_scalar('vae_' + name, loss, epoch) \n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # 1.python 中命令行参数解析\n",
        "    parser = argparse.ArgumentParser() \n",
        "    parser.add_argument('--cfg', default=None)\n",
        "    parser.add_argument('--mode', default='train')\n",
        "    parser.add_argument('--test', action='store_true', default=False) #store_true就代表着一旦有这个参数，做出动作“将其值标为True”，也就是没有时，默认状态下其值为False 对于boolea类型的值。\n",
        "    parser.add_argument('--iter', type=int, default=0)\n",
        "    parser.add_argument('--seed', type=int, default=0)\n",
        "    parser.add_argument('--gpu_index', type=int, default=0)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    \"\"\"setup\"\"\"\n",
        "    \"\"\"\n",
        "        随机种子\n",
        "        参数类型\n",
        "        训练设备\n",
        "        可视化\n",
        "        日志文件\n",
        "    \"\"\"\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed) # 固定随机种子，每次重新运行产生的随机数相同\n",
        "\n",
        "    dtype = torch.float64\n",
        "    torch.set_default_dtype(dtype) # 设置默认参数类型\n",
        "\n",
        "    device = torch.device('cuda', index=args.gpu_index) if torch.cuda.is_available() else torch.device('cpu') # 选择训练设备\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.set_device(args.gpu_index)\n",
        "    cfg = Config(args.cfg, test=args.test) # 获取配置类\n",
        "    tb_logger = SummaryWriter(cfg.tb_dir) if args.mode == 'train' else None  # tensorboard创建，如果是train过程，就可视化\n",
        "    logger = create_logger(os.path.join(cfg.log_dir, 'log.txt')) # 创建日志文件\n",
        "\n",
        "    \"\"\"parameter\"\"\"\n",
        "    \"\"\"\n",
        "      mode: train or test\n",
        "      nz: \n",
        "      t_his: 历史时间步长\n",
        "      t_pred: 待预测时间步长\n",
        "    \"\"\"\n",
        "    mode = args.mode # train or test\n",
        "    nz = cfg.nz\n",
        "    t_his = cfg.t_his\n",
        "    t_pred = cfg.t_pred\n",
        "\n",
        "    \"\"\"data\"\"\" # 加载数据\n",
        "    dataset_cls = DatasetH36M if cfg.dataset == 'h36m' else DatasetHumanEva\n",
        "    dataset = dataset_cls('train', t_his, t_pred, actions='all', use_vel=cfg.use_vel) # 初始化 dataset类 加载数据\n",
        "    if cfg.normalize_data:\n",
        "        dataset.normalize_data() # 是否对数据进行归一化\n",
        "\n",
        "    \"\"\"model\"\"\"\n",
        "    \"\"\"\n",
        "      model: 获取VAE模型\n",
        "      optimizer: 优化器\n",
        "      scheduler: 动态设置学习率\n",
        "    \"\"\"\n",
        "    model = get_vae_model(cfg, dataset.traj_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.vae_lr)\n",
        "    scheduler = get_scheduler(optimizer, policy='lambda', nepoch_fix=cfg.num_vae_epoch_fix, nepoch=cfg.num_vae_epoch) # 设置学习率\n",
        "\n",
        "    # 加载预训练模型\n",
        "    if args.iter > 0:\n",
        "        cp_path = cfg.vae_model_path % args.iter\n",
        "        print('loading model from checkpoint: %s' % cp_path)\n",
        "        model_cp = pickle.load(open(cp_path, \"rb\")) # 读取数据\n",
        "        model.load_state_dict(model_cp['model_dict']) # 模型加载参数\n",
        "\n",
        "    # 训练过程\n",
        "    if mode == 'train':\n",
        "        model.to(device) # 将模型移到GPU上\n",
        "        model.train() # 启用 batch normalization 和 dropout\n",
        "        for i in range(args.iter, cfg.num_vae_epoch):\n",
        "            train(i) # 训练模型\n",
        "            if cfg.save_model_interval > 0 and (i + 1) % cfg.save_model_interval == 0:\n",
        "                with to_cpu(model): # 将模型移到CPU中，准备保存中间数据。\n",
        "                    cp_path = cfg.vae_model_path % (i + 1) # 模型保存路径\n",
        "                    model_cp = {'model_dict': model.state_dict(), 'meta': {'std': dataset.std, 'mean': dataset.mean}}\n",
        "                    pickle.dump(model_cp, open(cp_path, 'wb'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1fq0Nc6Egvs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. TensorBoard\n",
        "https://www.cnblogs.com/sddai/p/14516691.html"
      ],
      "metadata": {
        "id": "z_ZTf-b4lVV4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwQ2L-sytW1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.如何动态学习率"
      ],
      "metadata": {
        "id": "4y7W20IdlZRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. 如何加载预训练模型\n",
        "pickle类"
      ],
      "metadata": {
        "id": "mey6SW1jlqxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. 一些函数学习"
      ],
      "metadata": {
        "id": "zDns8Pysp8JT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.randn(2, 3, 5)\n",
        "y = torch.permute(x, (2, 0, 1)).contiguous()\n",
        "print(x.size())\n",
        "print(y.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4N0tawnpED3",
        "outputId": "eaa32a7d-98c2-418f-f8eb-2119c8630c47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 5])\n",
            "torch.Size([5, 2, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 transpose、permute等维度变换操作后，tensor在内存中不再是连续存储的，而view操作要求tensor的内存连续存储，所以需要contiguous来返回一个contiguous copy；\n",
        "\n",
        "2 维度变换后的变量是之前变量的浅拷贝，指向同一区域，即view操作会连带原来的变量一同变形，这是不合法的，所以也会报错；---- 这个解释有部分道理，也即contiguous返回了tensor的深拷贝contiguous copy数据；"
      ],
      "metadata": {
        "id": "dEif8p4Qp1Ed"
      }
    }
  ]
}